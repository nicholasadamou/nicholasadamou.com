---
title: "How I Developed IBM's Direct File Input (DFI) Service for Global Ledger and Boosted NPS by 90 Points"
date: "2021-01-01"
summary: "Architecting the DFI Service for IBM's Global Ledger Interface, supporting 24k+ documents quarterly and managing 150k files during peak periods."
tags: ["IBM", "Development", "GLUI NG", "DFI", "Spring Boot", "Apache POI", "Scalability"]
---

## Introduction

In the world of finance and accounting, efficiency, accuracy, and reliability are paramount. The IBM Global Ledger User Interface Next Generation (GLUI NG) project was conceived as a modernized, cloud-native tool to meet these needs, and a critical component of this initiative was the development of the Direct File Input (DFI) service. This service was designed to handle the complexities of managing vast quantities of ledger journals, processing over 24,000 documents per quarter, and managing up to 150,000 files during year-end and quarterly close periods.

## Business Opportunity

The existing Global Direct File Input (GDFI) tool, part of the legacy GLUI application, was based on an Excel macro program that posed several challenges. The lack of control, customizability, and error handling made the tool cumbersome and error-prone. Users struggled with ensuring they had the latest version of the Excel macro, navigating the UI, and handling errors within their files. These issues highlighted the need for a more robust, user-friendly solution within the GLUI NG application.

The solution was twofold: first, to allow users to generate a customized DFI Excel template based on specific fields such as country, source, and MVS ID; and second, to provide a means to upload and process these templates within GLUI NG. This would enable users to efficiently create and process ledger journals while maintaining control and minimizing errors.

## Developer Contribution

As the lead developer for this project, I was responsible for both the front-end and back-end processes that powered the next-generation DFI features in GLUI. My focus was on ensuring performance, code quality, and maintainability throughout the development process.

Key technologies I employed included:

- **Apache POI** for handling the generation and parsing of Excel files.
- **Spring Boot** for managing the service's API.
- **Carbon Design Framework** for the user interface.
- **IBM Turbonomics** for optimizing resource allocation and ensuring the scalability of the DFI service.
- **IBM Hybrid Cloud Cirrus** for seamless integration and deployment across hybrid cloud environments.
- **IBM DB2 Database** for reliable and scalable data storage and retrieval, crucial for managing the large volumes of financial data processed by the DFI service.

The DFI service was built with scalability in mind, utilizing multi-threading to handle a large number of concurrent requests efficiently. This allowed the service to support GLUI's active user base during peak periods. Additionally, I implemented the data access object pattern to separate low-level data operations from high-level services, ensuring the maintainability and extensibility of the service.

## Scalability and Performance

To handle the high volume of documents and files processed by the DFI service, we leveraged several IBM technologies:

- **IBM Turbonomics**: This tool was instrumental in optimizing resource allocation for the DFI service. By analyzing the workloads and automatically adjusting resources, Turbonomics ensured that the service could scale efficiently to meet demand during peak periods, such as year-end or quarterly closes.

- **IBM Hybrid Cloud Cirrus**: The DFI service was deployed across a hybrid cloud environment using IBM Hybrid Cloud Cirrus. This enabled seamless integration with other IBM cloud services and on-premise systems, ensuring that the DFI service could operate at scale without compromising on performance or reliability.

- **IBM DB2 Database**: The DFI service relied on IBM DB2 for its database needs, providing robust and scalable storage for the vast amounts of financial data processed. DB2's advanced features, such as data compression and partitioning, helped maintain high performance even as the volume of data grew.

These technologies played a crucial role in ensuring that the DFI service could handle the scale required by IBM's global financial operations. By integrating these tools into the architecture, we created a system that could not only manage the current load but also scale to meet future demands.

## Future Plans: Implementing Blue-Green Deployments

Looking ahead, I plan to implement blue-green deployments for the DFI service as part of our continuous integration and continuous deployment (CI/CD) strategy. Blue-green deployments involve maintaining two identical production environments—one active (blue) and one idle (green). During a deployment, the new version of the application is deployed to the idle environment (green). Once validated, traffic is switched from the active environment (blue) to the idle one, making it the new active environment. This approach offers several key benefits:

1. **Minimized Downtime**: By deploying the new version of the application in the idle environment and switching traffic only after validation, blue-green deployments significantly reduce downtime. Users experience minimal disruption, ensuring that critical financial operations can continue without interruption.

2. **Risk Mitigation**: If an issue is detected after switching traffic to the new environment, it's straightforward to roll back to the previous version by simply switching traffic back to the original environment. This reduces the risk associated with deployments and provides a quick recovery option in case of failures.

3. **Improved Testing and Validation**: Blue-green deployments allow for thorough testing and validation of new features in a production-like environment before they go live. This ensures that any issues can be caught early, improving the quality and reliability of the DFI service.

4. **Scalability**: With blue-green deployments, we can also test how the new version of the service performs under load in the green environment before fully switching over. This helps in fine-tuning performance and ensuring that the service can handle the expected scale.

By implementing blue-green deployments, I aim to further enhance the robustness and reliability of the DFI service, making it even more resilient and responsive to the needs of IBM's global financial operations.

## Impact

The successful delivery of the DFI feature significantly improved the user experience for those processing ledger journals. It addressed the pain points of the previous GDFI system, providing users with a more intuitive and reliable tool. The enhanced DFI system in GLUI NG has led to increased adoption, with users preferring it over the older BAU implementation.

Following the rollout of the new DFI service, I conducted user surveys after users interacted with the feature for the first time. The feedback was overwhelmingly positive, with our Net Promoter Score (NPS) increasing from -30 to +60, a significant improvement that underscored the value of the new system. This jump in NPS reflected the enhanced efficiency, ease of use, and reliability of the DFI service, which resolved many of the issues that users had faced with the legacy system.

## Lessons Learned

### Early Technical Challenges

One of the most significant challenges encountered during the development of the DFI service was the uncertainty surrounding the feasibility of generating Excel files that met the complex requirements of the feature. This process had not been attempted in GLUI’s codebase before, so early research and the development of a proof of concept were critical. By validating the technical approach early on, I was able to mitigate risks and build the higher-level features with greater confidence and efficiency.

### Handling Non-Standard User Input

A particularly challenging aspect of the project was managing non-standard user input. Users often provided data that did not conform to expected formats, such as non-standard characters, spaces, or copied and pasted data that bypassed validation rules. This issue was not only difficult to plan for but also to detect and handle effectively.

To address this, I implemented multiple layers of validation. First, I utilized Excel’s built-in functionality to prevent users from entering invalid data directly into cells. However, this was not foolproof, as users could still bypass these rules by copying and pasting data. To counter this, I added additional checks on the server side to generalize and validate the data before processing it. This approach reduced the likelihood of errors and ensured that the system could handle a wide range of edge cases gracefully.

### Excel File Compatibility Issues

During the development process, I encountered an issue related to the compatibility of different Excel file formats (XLS vs. XLSX). Initially, I assumed that all incoming Excel documents would be of the newer XLSX type (XSSF), which led to errors when users submitted older XLS files (HSSF). This assumption caused the system to fail when processing these files.

To resolve this, I modified the code to handle both XSSF and HSSF formats by using more generic types within Apache POI, such as `Workbook`, `Sheet`, and `Cell`. This change ensured that the service could process both modern and legacy Excel files without errors. The lesson learned here was to avoid making assumptions about file types and to utilize polymorphism to create a more flexible and robust solution.

### External Add-Ons and Hidden Sheets

Another unexpected issue arose from the use of external Excel add-ons, such as IBM Planning Analytics, which introduced hidden sheets into the DFI Excel files. These hidden sheets, when parsed as the first sheet in the document, contained no data, causing the service to fail.

To address this, I modified the parsing logic to target specific sheet names rather than relying on the order of sheets within the document. If the expected sheet was not found, the system would return a meaningful error to the user, guiding them to correct the issue. This experience highlighted the importance of not assuming that the first sheet in an Excel document is the one to be parsed, especially when dealing with complex, enterprise-level applications.

### Technology Quirks and Testing

While working with Apache POI, I encountered quirks related to how

 Excel handles seemingly empty rows. For example, a row that had previously contained data but was later cleared could still be counted as a valid row, leading to errors in our processing logic. This issue was particularly challenging to detect, as it only occurred in rare cases that were not caught during the extensive testing phases.

The lesson learned here was the importance of thoroughly understanding the behavior of the tools and libraries used in development. Even well-documented functions can have edge cases or unexpected behaviors that can lead to issues in production. As a result, I placed a greater emphasis on testing for edge cases and carefully reviewing the documentation to anticipate potential problems.

## Conclusion

The development of the DFI service for IBM's GLUI NG was a complex but rewarding project that significantly enhanced the functionality and usability of the Global Ledger Interface. By focusing on performance, scalability, maintainability, and user experience, I delivered a solution that not only met but exceeded the needs of its users. The integration of technologies such as IBM Turbonomics, IBM Hybrid Cloud Cirrus, and IBM DB2 Database ensured that the service could scale effectively and handle the demands of IBM's global financial operations.

The planned implementation of blue-green deployments will further enhance the resilience and reliability of the DFI service, providing a robust framework for future updates and expansions. The increase in NPS from -30 to +60 after the introduction of the DFI service is a testament to the positive impact of the project. The lessons learned from this project, including the importance of early validation, handling non-standard input, ensuring compatibility with legacy formats, and thoroughly understanding the technology stack, will guide future developments in GLUI and similar projects. This experience has reinforced the value of careful planning, rigorous testing, and continuous learning in delivering high-quality software solutions.
